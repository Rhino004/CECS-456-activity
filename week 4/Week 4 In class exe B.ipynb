{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334bdffe",
   "metadata": {},
   "source": [
    "# 1. Bias-Variance Tradeoff, Overfitting & Underfitting\n",
    "You are working on a predictive model to detect fraudulent transactions using a decision tree classifier. Initially, you trained a very deep tree that performs well on the training set but poorly on new data. To address this, you tried a shallow tree, but its performance was suboptimal across both training and test sets.\n",
    "\n",
    "Questions:\n",
    "- How would you describe the bias-variance tradeoff in this situation?\n",
    "  The bias-variance is the balance between underfitting and overfitting.\n",
    "- Which of the models is suffering from high variance, and which one has high bias?\n",
    "  High variance is in overfitting because the deep decision tree has low training error but high test error and high bias appers in underfitting because the shallow decision tree has high error on both training and test sets.\n",
    "- What strategies can you use to achieve a balance between bias and variance?\n",
    "  To achieve balance we would have to select meaningful features, remove noise, and creat new informative features to improve model generalization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801a635",
   "metadata": {},
   "source": [
    "# 2. Accuracy, Precision, Recall, and Error Rate\n",
    "A healthcare startup is building a machine learning model to predict whether a patient has a rare disease based on their medical records. The dataset is highly imbalanced, with only 2% of cases being positive. Your team trained a logistic regression model and reported 98% accuracy.\n",
    "\n",
    "Questions:\n",
    "- Why is accuracy not the best metric in this case?\n",
    "-  accuracy is not the best metric because of the imbalanced datasets because it can be misleading.\n",
    "- How would you calculate and interpret precision and recall in this scenario?\n",
    "-  we would calculate the precision with true positives divided by true positives plues false positives and we can understand that many of the predicted postives cases are actually. For recall we would true positives divided by true positives plues false negatives. This measures how many actual positives cases were correctly identified.\n",
    "- If the recall is 70% and precision is 30%, what does this indicate about the model’s performance?\n",
    "-  This means that we have an unbalance dataset and the model prioritizes high recall but at the cost of many incorrect predictions. This could lead to unnecessary medical tests and anxiety for patients.\n",
    "- What steps would you take to improve the model’s predictive ability?\n",
    "-  I would extract relevant medical indicators to improve model accuracy and remove noisy or redundant featrues that contribute to false positives. Another thing I would try to do is balance the dataset with sampling\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7afad",
   "metadata": {},
   "source": [
    "# 3. Area Under the ROC Curve (AUC-ROC)\n",
    "You built two classifiers to detect spam emails: Model A (a random forest) and Model B (a k-NN model). After evaluation, Model A has an AUC-ROC score of 0.85, while Model B has a score of 0.65.\n",
    "\n",
    "Questions:\n",
    "- What does the AUC-ROC score indicate about the performance of each model?\n",
    "-  The AUC-ROC is the accuray of the model becuase in this cause it's the model's ability to distinguish between spam and non-spam emails.\n",
    "- If you were to deploy one of the models in a real-world email filtering system, which would you choose and why?\n",
    "-  I would deploy model A because it's AUC-ROC value is higher than model B and will be distinguished emails and spams.\n",
    "- Suppose Model B’s AUC-ROC improves to 0.75 after hyperparameter tuning. How would you further assess whether it is ready for deployment?\n",
    "- I would assess the model by compare precision, recall and f1-score along with that I would check false positive and false negatives rates.Final to see if it is ready for deployment I would conduct real-world testing to collect data on the performances of the mdoel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d6157",
   "metadata": {},
   "source": [
    "# 4. k-NN Distance Metrics and Feature Scaling\n",
    "Scenario: You are working with a k-NN classifier on a dataset with mixed features, including age (years), income ($), and number of purchases. After training the model, you notice that income has a dominant effect on the distance calculations.\n",
    "\n",
    "Questions:\n",
    "- Why does income have a dominant effect on k-NN distance calculations?\n",
    "- In k-NN is the distance calculation determine the closeness of data points and income has dominant can effect because it has a much larger numerical range than the other features.\n",
    "- How does feature scaling (e.g., Min-Max Scaling, Standardization) help in k-NN?\n",
    "- Scaling ensures that all features contribute equally to distance calculations and preventing features with larger rnages form dominating.\n",
    "- Suppose you are using Euclidean distance. Would feature scaling be necessary? Why or why not?\n",
    "- Even with Eclidean distance we would still use scaling becuase features with large values will dominate the distance calculation, reducing the impact of smaller-scaled features like age.\n",
    "- If the dataset contains categorical variables, how can k-NN handle them?\n",
    "- K-NN requires a numerical representation of all features. This means that categorical variables must be encoded before applying distance based calculation. K-NN can handle them with One-Hot encoding or Binary Encoding.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d7eeb",
   "metadata": {},
   "source": [
    "# 5. Choosing the Optimal k Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define range for k values\n",
    "k_values = list(range(1, 21))\n",
    "cv_scores = []\n",
    "#haing low k is about high varances and \n",
    "# Perform cross-validation\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Select the best k\n",
    "best_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Best k: {best_k}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a0fd4-99b9-4af3-9bb9-30982e60f1c9",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What does `cross_val_score` do in this code?\n",
    "- 'cross_val_score' performs the cross-balidation and this is to find a balances of overfeeding and underfeeding\n",
    "- Why do we perform 5-fold cross-validation to choose `k`?\n",
    "- We do this to balances the overfitting and underfitting and to reduces variance in evaluation\n",
    "- What does the `np.argmax(cv_scores)` function return, and why is it useful?\n",
    "- This function returns the index of the maximum 'cross_val_score' which is to find the best 'k' value. We use this function to help find the optimal 'k' that yields the best performance.\n",
    "- What happens if `k` is set too high? How would it affect bias and variance?\n",
    "- If the 'k' is to high it would affect the bias of the model. This is because the model becoems overly simplistic. However the variance would be more stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bf220",
   "metadata": {},
   "source": [
    "# 6. Effect of Different Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d02dc-19fc-460d-9709-c03414e639f3",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What is the difference between Euclidean and Manhattan distances?\n",
    "- the difference is in the process of finding the distance measure between two point. Euclidean does the straight-line distance and Manhattan is the absolute differences between coordinates.\n",
    "- In which cases would Manhattan distance be preferable over Euclidean?\n",
    "- A cases would be in a high dimensional dataset because Euclidean distance can be distorted by irrelevant dimensions.\n",
    "- How would using Minkowski distance generalize both Euclidean and Manhattan distances?\n",
    "- Its because Minkowski distances includes both Euclidean and Manhattan distances.\n",
    "- How do different distance metrics affect k-NN classification in high-dimensional data?\n",
    "- Euclidean becomes less effective in high dimensions due to the curse of dimensionality—distances tend to become similar. Manhattan is mroe robust in high-dimensional space because it does not amplify large differences in a single feature. Lastly, minkowski is felxbable in choosing a distance metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d5a34-43c0-4c05-95d9-8ae56cf8a8e9",
   "metadata": {},
   "source": [
    "# 7. Handling Imbalanced Data in k-NN\n",
    "\n",
    "Scenario: You are applying k-NN to a medical dataset where 95% of patients are healthy (negative class) and only 5% have a rare disease (positive class). The model achieves high accuracy but fails to detect positive cases.\n",
    "\n",
    "Questions:\n",
    "- Why does k-NN struggle with imbalanced datasets?\n",
    "- It's because k-NN relies on majority voting among the nearest neighbors.\n",
    "- What are some techniques to handle imbalanced classes in k-NN? (e.g., weighted voting, SMOTE)\n",
    "- The techniques that can be used to hangle imbalanced classes is to adjust the wight of the tree by using oversampling or undersampling.\n",
    "- How does setting the `weights='distance'` parameter in `KNeighborsClassifier` help in this scenario?\n",
    "- `weights='distance' ensures that the closer neighbors have mroe influence and this reduces the impact of faraway negative class samples.\n",
    "- If you were to use precision-recall curves instead of accuracy, how would that impact model evaluation?\n",
    "- The impact would be on the accuracy because it would be misleading in imbalanced datasets because predicting only negative cases gives 95% accuracy, despite failing to detect any positives cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795f405-9e09-4a56-94a5-0157d37a0d6a",
   "metadata": {},
   "source": [
    "# 8. k-NN vs. Tree-Based Classifiers\n",
    "\n",
    "Scenario: You need to classify images into categories, and you are choosing between k-NN and Decision Trees.\n",
    "\n",
    "Questions:\n",
    "- What are the main advantages of k-NN compared to tree-based classifiers like Decision Trees or Random Forests?\n",
    "- The advantage of k-NN is that it makes no assumptions about data distribution making it useful for complex irregular patterns\n",
    "- When would tree-based models be a better choice than k-NN?\n",
    "- The tree-based models would be better at handling  missing data or when the dataset has outliers.\n",
    "- k-NN requires storing all training data for predictions. How does this impact its computational efficiency compared to tree-based classifiers?\n",
    "- while tree-based classifiers require more computation during training but make faster predictions once trained. However, k-NN is very slow for large datasets because it must compute distances to all training samples for every new prediction.\n",
    "- How would a high-dimensional feature space affect k-NN’s performance?\n",
    "- In a high-dimensional feature the computational cost increases because the distance calculations become expensive due to the large number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cda8bc-7ea9-4a55-9b58-54096482f487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
